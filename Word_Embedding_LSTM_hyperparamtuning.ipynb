{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Word_Embedding_LSTM_hyperparamtuning.ipynb","provenance":[{"file_id":"1D-OljKnp3xdclEbvGqj6FuY8XO_TTglj","timestamp":1584020332575}],"collapsed_sections":[]},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"UjKxHrLEzqlE","colab":{}},"source":["# This code is based on a tutorial which can be found here:\n","# https://realpython.com/python-keras-text-classification/#what-is-a-word-embedding"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MW3L1Loy4Z2Y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1593081074525,"user_tz":-120,"elapsed":1393,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"39147a12-4ac6-42e7-c2ff-2e3f6e1c021e"},"source":["# %matplotlib inline\n","# import matplotlib.pyplot as plt\n","# import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","from tensorflow.keras import backend as k\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, GRU, Embedding, Dropout, LSTM, Bidirectional\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.models import model_from_json\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","\n","import pickle\n","\n","# print(tf.__version__)\n","# print(tf.keras.__version__)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nnVjGsno7q-d","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":844},"executionInfo":{"status":"ok","timestamp":1593081078550,"user_tz":-120,"elapsed":5367,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"8bdabf08-12b9-4b52-f9cd-27f6ba177b83"},"source":["#Packages to import (regular expressions and pandas)\n","\n","import re \n","import pandas as pd \n","\n","from pathlib import Path\n","import gzip\n","import json\n","\n","# Code to read csv file into Colaboratory:\n","!pip install -U -q PyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","#Mount google colab to google drive so it can access files\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#The huge gzipped JSON files ('NL_jobs.15.jsonl.gz') are located in this folder\n","cwd = '/content/drive/My Drive/Werkinzicht_Clyde/Notebooks/Experiments'\n","\n","#Print the content of the folder\n","!ls '/content/drive/My Drive/Werkinzicht_Clyde/Notebooks/Experiments'\n","\n","# set current working directory, This folder should only contain the huge json files.\n","%cd '/content/drive/My Drive/Werkinzicht_Clyde/Notebooks/Experiments'\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","'Baseline Model (BOW Log. regression).ipynb'\n"," BERT_alt_deploy.ipynb\n","'BERT_Jads_Nikita- Clyde edit - hpartuning.ipynb'\n"," BOW_LSTM.ipynb\n","'Copy of Copy of BERT_alt _ hyperparamtuning.ipynb'\n"," CORPUSNL_jobs.15.jsonl.gz_JSON_splitted.tsv_1.tsv\n"," Cow\n"," COW_LSTM_hyperparamtuning.ipynb\n"," dev.gsheet\n"," dev.tsv\n"," df3_updated_clean.txt\n"," df3_updated.txt\n"," df4_updated_clean.txt\n"," df4_updated.txt\n"," Exp23_RNDSCV.csv\n"," Exp25_RNDSCV.csv\n"," Exp26_RNDSCV.csv\n"," Exp27_RNDSCV.csv\n"," Exp28_RNDSCV.csv\n"," Exp29_RNDSCV.csv\n"," Exp30_RNDSCV.csv\n"," Exp31_RNDSCV.csv\n"," Exp32_RNDSCV.csv\n"," Exp33_RNDSCV.csv\n"," Exp34_RNDSCV.csv\n"," Exp35_RNDSCV.csv\n"," Exp36_RNDSCV.csv\n"," fastText-0.1.0\n"," glove.6B\n"," glove.6B.zip\n"," glove.model\n"," model.h5\n"," model.json\n"," NL_Embedding\n"," NL_jobs.15.jsonl.gz_JSON_splitted.tsv\n"," temp\n"," tokenizer.pickle\n"," train.gsheet\n"," train.tsv\n"," v0.1.0.zip\n"," v0.1.0.zip.1\n"," v0.1.0.zip.2\n"," Word2Vec_LSTM_hyperparamtuning.ipynb\n"," Word_Embedding_LSTM_hyperparamtuning.ipynb\n"," Word_Embedding_LSTM.ipynb\n","/content/drive/My Drive/Werkinzicht_Clyde/Notebooks/Experiments\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q176l_o_4Z2e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593081078553,"user_tz":-120,"elapsed":5342,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"0088ac00-9056-4de7-aede-5f2adb8aef59"},"source":["TRAIN_FILE = \"train.tsv\"\n","DEV_FILE = \"dev.tsv\"\n","\n","CORPUS_FILE = \"CORPUSNL_jobs.15.jsonl.gz_JSON_splitted.tsv_1.tsv\"\n","\n","print(\"TRAIN FILE: \\t\\t{}\\nDEVELOPMENT FILE: \\t{}\".format(TRAIN_FILE, DEV_FILE))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TRAIN FILE: \t\ttrain.tsv\n","DEVELOPMENT FILE: \tdev.tsv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bvHjyC5o4Z2l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1593081078555,"user_tz":-120,"elapsed":5317,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"72f9144a-8fa6-4873-e928-d740725e0c5e"},"source":["# Load the data\n","test_full = pd.read_csv(TRAIN_FILE, sep = \"\\t\")\n","dev_full = pd.read_csv(DEV_FILE, sep = \"\\t\")\n","\n","# Tirm the data to only one we need\n","test_trim = test_full[[\"Quality\", \"#1 String\"]]\n","dev_trim = dev_full[[\"Quality\", \"#1 String\"]]\n","\n","# Seperate our data for training and testing\n","train_text = np.array(test_full[\"#1 String\"])\n","train_target = np.array(test_full[\"Quality\"])\n","\n","test_text = np.array(dev_full[\"#1 String\"])\n","test_target = np.array(dev_full[\"Quality\"])\n","\n","# Check our training and testing sets\n","print(\"Train-set size: \", len(train_text))\n","print(\"Test-set size:  \", len(test_text))\n","\n","# Stack text for tokenization\n","data_text = np.hstack((train_text, test_text))\n","print(\"Shape of the stacked text: {}\".format(data_text.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train-set size:  2240\n","Test-set size:   720\n","Shape of the stacked text: (2960,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bbhITV8B4Z2w","colab_type":"code","colab":{}},"source":["# Get stop words (or most commoon ones basically) for the dutch language\n","stop_words = set(stopwords.words('dutch'))\n","\n","# Get the tokinized pattern of all sentences\n","word_tokens = []\n","for sentence in data_text:\n","    word_tokens.append(nltk.word_tokenize(sentence))\n","\n","# Initiate a new collector\n","filtered_sentence = []\n","for entry in word_tokens:\n","    filtered_sentence.append([w for w in entry if not w in stop_words and w.isalpha()])\n","filtered_sentence = np.array(filtered_sentence)\n","\n","# Implement the top level tokinizer, we will go for 10K words\n","num_words = 10000\n","tokenizer = Tokenizer(num_words=num_words)\n","tokenizer.fit_on_texts(filtered_sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VBDiFAm4Z21","colab_type":"code","colab":{}},"source":["# tokenizer.word_index # If we need to check it"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fwqp-UPr4Z26","colab_type":"code","colab":{}},"source":["# Save the tokinizer for re-usability with the new data\n","import pickle\n","with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vpzkYm3y4Z2-","colab_type":"code","colab":{}},"source":["# loading\n","with open('tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhsCkjh74Z3F","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":263},"executionInfo":{"status":"ok","timestamp":1593081079436,"user_tz":-120,"elapsed":6056,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"11749428-8abe-4799-ed0c-7708ce4aa711"},"source":["x_train_tokens = tokenizer.texts_to_sequences(train_text)\n","x_test_tokens = tokenizer.texts_to_sequences(test_text)\n","\n","# Example of conversions\n","print(\"Conversion Examples:\")\n","print(x_train_tokens[50])\n","print(train_text[50])\n","print(\"---------------------------------\")\n","print(x_train_tokens[100])\n","print(train_text[100])\n","print(\"---------------------------------\")\n","\n","# Padding the data if needed\n","num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n","num_tokens = np.array(num_tokens)\n","print(\"Average Number of Tokens per sentence: \", np.round(np.mean(num_tokens), 4))\n","print(\"Maxmimum Number of Tokens per sentence: \", np.max(num_tokens))\n","print(\"Minimum Number of Tokens per sentence: \", np.min(num_tokens))\n","\n","# The max number of tokens we will allow is set to the average plus 2 standard deviations.\n","max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n","max_tokens = int(max_tokens)\n","print(\"Maximum amount of tokens to hold: \", max_tokens)\n","\n","# This covers almost 96% of the data\n","print(\"How much data does the padding include fully: {}\".format(np.sum(num_tokens < max_tokens) / len(num_tokens)))\n","\n","# Initiate the condition for padding\n","pad = 'pre'\n","\n","# Applying padding to the training data\n","x_train_pad = pad_sequences(x_train_tokens,\n","                            maxlen=max_tokens,\n","                            padding=pad,\n","                            truncating=pad)\n","\n","# Applying padding to the development data\n","\n","x_test_pad = pad_sequences(x_test_tokens, \n","                           maxlen=max_tokens, \n","                           padding=pad, \n","                           truncating=pad)\n","\n","# Verify that all of the padding is done correctly\n","print(\"Shape of the padded testing data: \\t\", x_train_pad.shape)\n","print(\"Shape of the padded development data: \\t\", x_test_pad.shape)\n","# Seems correct"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Conversion Examples:\n","[233, 2386, 245, 2387]\n","Utrecht, Drenthe en Flevoland.\n","---------------------------------\n","[13, 152, 614, 298, 1211, 2416, 298, 2417, 462, 237, 298, 2418, 835, 115, 823, 2419, 710]\n","het, met behulp van ons calculatieprogramma BV6 van TSD uit Zwolle, uitwerken van prijsaanvragen tot een correcte werkomschrijvende offerte\n","---------------------------------\n","Average Number of Tokens per sentence:  12.7966\n","Maxmimum Number of Tokens per sentence:  58\n","Minimum Number of Tokens per sentence:  0\n","Maximum amount of tokens to hold:  29\n","How much data does the padding include fully: 0.9618243243243243\n","Shape of the padded testing data: \t (2240, 29)\n","Shape of the padded development data: \t (720, 29)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nPbTfaGOlLto","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EGb1lhl4Z3I","colab_type":"code","colab":{}},"source":["# def create_model(lstm1, lstm2, dense1, vocab_size, embedding_dim, maxlen):\n","#     #Creating the network\n","#     # embedding_size = 256\n","#     # num_words = num_words # We chose this value earlier (10,000)\n","#     # max_tokens = max_tokens # This values is also chosen from before (30)\n","\n","#     # For reproducability setting a seed\n","#     seed = 42\n","#     # np.random.seed(seed)\n","\n","#     model = Sequential()\n","#     model.add(Embedding(input_dim=vocab_size,\n","#                         output_dim=embedding_dim,\n","#                         input_length=maxlen,\n","#                         name='layer_embedding'))\n","#     model.add(Bidirectional(LSTM(units = 32, return_sequences=True, name = \"BiLSTM_1\")))\n","#     model.add(Dropout(0.5, noise_shape=None, seed=seed))\n","#     model.add(Bidirectional(LSTM(units = 16, return_sequences=False, name = \"BiLSTM_2\")))\n","#     model.add(Dropout(0.5, noise_shape=None, seed=seed))\n","#     model.add(Dense(16, activation='relu', name = \"Intermediate_1\"))\n","#     Dropout(rate = 0.5, noise_shape=None, seed=seed)\n","#     # model.add(Dense(16, activation='relu', name = \"Intermediate_2\"))\n","#     model.add(Dense(1, activation='sigmoid', name = \"Final_Output\"))\n","#     optimizer = Adam(lr = 1e-3)\n","#     model.compile(loss='binary_crossentropy',\n","#                   optimizer=optimizer,\n","#                   metrics=['acc'])\n","#     # model.summary()\n","\n","#     return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YEHyPsAJeRsF","colab_type":"code","colab":{}},"source":["def create_model(lstm1, lstm2, dense1, vocab_size, embedding_dim, maxlen):\n","    #Creating the network\n","    # embedding_size = 256\n","    # num_words = num_words # We chose this value earlier (10,000)\n","    # max_tokens = max_tokens # This values is also chosen from before (30)\n","\n","    # For reproducability setting a seed\n","    seed = 42\n","    # np.random.seed(seed)\n","\n","    model = Sequential()\n","    model.add(Embedding(input_dim=vocab_size,\n","                        output_dim=embedding_dim,\n","                        input_length=maxlen,\n","                        name='layer_embedding'))\n","    model.add(Bidirectional(LSTM(units = lstm1, return_sequences=True, name = \"BiLSTM_1\")))\n","    model.add(Dropout(0.5, noise_shape=None, seed=seed))\n","    model.add(Bidirectional(LSTM(units = lstm2, return_sequences=False, name = \"BiLSTM_2\")))\n","    model.add(Dropout(0.5, noise_shape=None, seed=seed))\n","    model.add(Dense(dense1, activation='relu', name = \"Intermediate_1\"))\n","    Dropout(rate = 0.5, noise_shape=None, seed=seed)\n","    # model.add(Dense(16, activation='relu', name = \"Intermediate_2\"))\n","    model.add(Dense(1, activation='sigmoid', name = \"Final_Output\"))\n","    optimizer = Adam(lr = 1e-3)\n","    model.compile(loss='binary_crossentropy',\n","                  optimizer=optimizer,\n","                  metrics=['acc'])\n","    model.summary()\n","\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dhNAMsIyCokj","colab_type":"code","colab":{}},"source":["# param_grid = dict(lstm1=[4,8,16, 32],\n","#                   lstm2=[4, 8,16, 32],\n","#                   dense1=[4, 8,16, 32],          \n","#                   vocab_size=[5000], \n","#                   embedding_dim=[50, 30, 80],\n","#                   maxlen=[29,50])\n","\n","param_grid = dict(lstm1=[4,8,16, 32],\n","                  lstm2=[4, 8,16, 32],\n","                  dense1=[4, 8,16, 32],          \n","                  vocab_size=[5000], \n","                  embedding_dim=[50, 30, 80],\n","                  maxlen=[29,50])\n","\n","# param_grid = dict(lstm1=[16, 32],\n","#                   lstm2=[16, 32],\n","#                   dense1=[16, 32],          \n","#                   vocab_size=[5000], \n","#                   embedding_dim=[50, 30, 80],\n","#                   maxlen=[29,50])\n","\n","# param_grid = dict(lstm1=[16, 32],\n","#                   lstm2=[16, 32],\n","#                   dense1=[16, 32],          \n","#                   vocab_size=[5000], \n","#                   embedding_dim=[50],\n","#                   maxlen=[29]\n","                  # ,\n","                  #  random_state=1, refit=True,\n","                  #  return_train_score=True\n","                  # )\n","\n","\n","# param_grid = dict(batch_size_param=[16, 32, 64, 128],\n","#                   lr_param=[1e-3, 5e-5, 3e-5, 2e-5],          \n","#                   vocab_size=[5000], \n","#                   embedding_dim=[50],\n","#                   maxlen=[29],       \n","#                    )\n","\n","\n","# param_grid = dict(num_filters=[32, 64, 128],\n","#                   kernel_size=[3, 5, 7],\n","#                   vocab_size=[vocab_size],\n","#                   embedding_dim=[embedding_dim],\n","#                   maxlen=[maxlen])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RyWEJZod9ASt","colab_type":"code","colab":{}},"source":["# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience = 5)\n","# model = KerasClassifier(build_fn=create_model,\n","#                         epochs=3, batch_size= 64,\n","#                         verbose=False,\n","#                         validation_data=(X_test, test_target))\n","#                         ,\n","#                         callbacks=[es])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iZTR9OshFgCz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":829},"executionInfo":{"status":"ok","timestamp":1593082167373,"user_tz":-120,"elapsed":1093882,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"7f4848de-6490-4507-97b0-9252c97053a9"},"source":["from keras.wrappers.scikit_learn import KerasClassifier\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","# Main settings\n","# epochs = 20\n","# embedding_dim = 50\n","# maxlen = 100\n","\n","output_file = 'data/output.txt'\n","\n","# Run grid search for each source (yelp, amazon, imdb)\n","# for source, frame in df.groupby('source'):\n","#     print('Running grid search for data set :', source)\n","#     sentences = df['sentence'].values\n","#     y = df['label'].values\n","\n","# # Train-test split\n","# sentences_train, sentences_test, y_train, y_test = train_test_split(\n","#     sentences, y, test_size=0.25, random_state=1000)\n","\n","# # Tokenize words\n","# tokenizer = Tokenizer(num_words=5000)\n","# tokenizer.fit_on_texts(sentences_train)\n","# X_train = tokenizer.texts_to_sequences(sentences_train)\n","# X_test = tokenizer.texts_to_sequences(sentences_test)\n","\n","# # Adding 1 because of reserved 0 index\n","# vocab_size = len(tokenizer.word_index) + 1\n","\n","# # Pad sequences with zeros\n","# X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n","# X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n","\n","# # Parameter grid for grid search\n","# param_grid = dict(num_filters=[32, 64, 128],\n","#                   kernel_size=[3, 5, 7],\n","#                   vocab_size=[vocab_size],\n","#                   embedding_dim=[embedding_dim],\n","#                   maxlen=[maxlen])\n","from sklearn.ensemble import GradientBoostingRegressor\n","from sklearn.metrics import fbeta_score, make_scorer\n","\n","\n","# Variables\n","epochs = 10 \n","cv_folds = 4\n","n_iter = 20\n","batch_size = 128\n","\n","model = KerasClassifier(build_fn=create_model,\n","                        epochs=epochs, batch_size= batch_size,\n","                        verbose=False\n","                       )\n","\n","grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,\n","                          cv= cv_folds, verbose=1, n_iter= n_iter,\n","                          # scoring= score,\n","                          return_train_score = True,\n","                          n_jobs = -1,\n","                          refit = True\n","                          )\n","\n","\n","\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Define early stopping\n","early_stopping = EarlyStopping(monitor='loss', patience=3)\n","\n","\n","grid_result = grid.fit(x_train_pad, train_target, callbacks=[early_stopping])\n","\n","# Evaluate testing set\n","test_accuracy = grid.score(x_test_pad, test_target)\n","\n","# # Save and evaluate results\n","# prompt = input(f'finished {source}; write to file and proceed? [y/n]')\n","# if prompt.lower() not in {'y', 'true', 'yes'}:\n","#     break\n","# with open(output_file, 'a') as f:\n","\n","    # s = ('Running {} data set\\nBest Accuracy : '\n","    #       '{:.4f}\\n{}\\nTest Accuracy : {:.4f}\\n\\n')\n","    # output_string = s.format(\n","    #     source,\n","    #     grid_result.best_score_,\n","    #     grid_result.best_params_,\n","    #     test_accuracy)\n","    # print(output_string)\n","    # f.write(output_string)\n","print(grid_result.best_score_)\n","print(grid_result.best_params_)\n","print(test_accuracy)\n","print(grid_result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Fitting 4 folds for each of 20 candidates, totalling 80 fits\n"],"name":"stdout"},{"output_type":"stream","text":["[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n","/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  \"timeout or by a memory leak.\", UserWarning\n","[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 10.0min\n","[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed: 17.9min finished\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","layer_embedding (Embedding)  (None, 50, 50)            250000    \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 50, 8)             1760      \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 50, 8)             0         \n","_________________________________________________________________\n","bidirectional_5 (Bidirection (None, 8)                 416       \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 8)                 0         \n","_________________________________________________________________\n","Intermediate_1 (Dense)       (None, 16)                144       \n","_________________________________________________________________\n","Final_Output (Dense)         (None, 1)                 17        \n","=================================================================\n","Total params: 252,337\n","Trainable params: 252,337\n","Non-trainable params: 0\n","_________________________________________________________________\n","WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"layer_embedding_input_2:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 29).\n","WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"layer_embedding_input_2:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 29).\n","WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"layer_embedding_input_2:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 29).\n","0.8102678507566452\n","{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 4, 'lstm1': 4, 'embedding_dim': 50, 'dense1': 16}\n","0.7777777910232544\n","RandomizedSearchCV(cv=4, error_score=nan,\n","                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fb9db8fb048>,\n","                   iid='deprecated', n_iter=20, n_jobs=-1,\n","                   param_distributions={'dense1': [4, 8, 16, 32],\n","                                        'embedding_dim': [50, 30, 80],\n","                                        'lstm1': [4, 8, 16, 32],\n","                                        'lstm2': [4, 8, 16, 32],\n","                                        'maxlen': [29, 50],\n","                                        'vocab_size': [5000]},\n","                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n","                   return_train_score=True, scoring=None, verbose=1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-F9onQ_F06Td","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1593082167377,"user_tz":-120,"elapsed":1093867,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"0707cae6-74ef-4887-e660-6328cbd7a742"},"source":["df_end = pd.DataFrame(grid_result.cv_results_)\n","df_end\n","# df_end.to_csv('Exp30_RNDSCV.csv')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mean_fit_time</th>\n","      <th>std_fit_time</th>\n","      <th>mean_score_time</th>\n","      <th>std_score_time</th>\n","      <th>param_vocab_size</th>\n","      <th>param_maxlen</th>\n","      <th>param_lstm2</th>\n","      <th>param_lstm1</th>\n","      <th>param_embedding_dim</th>\n","      <th>param_dense1</th>\n","      <th>params</th>\n","      <th>split0_test_score</th>\n","      <th>split1_test_score</th>\n","      <th>split2_test_score</th>\n","      <th>split3_test_score</th>\n","      <th>mean_test_score</th>\n","      <th>std_test_score</th>\n","      <th>rank_test_score</th>\n","      <th>split0_train_score</th>\n","      <th>split1_train_score</th>\n","      <th>split2_train_score</th>\n","      <th>split3_train_score</th>\n","      <th>mean_train_score</th>\n","      <th>std_train_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>26.881984</td>\n","      <td>0.329249</td>\n","      <td>2.335574</td>\n","      <td>0.053679</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>30</td>\n","      <td>4</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 16...</td>\n","      <td>0.776786</td>\n","      <td>0.810714</td>\n","      <td>0.807143</td>\n","      <td>0.750000</td>\n","      <td>0.786161</td>\n","      <td>0.024691</td>\n","      <td>10</td>\n","      <td>0.981548</td>\n","      <td>0.973810</td>\n","      <td>0.984524</td>\n","      <td>0.989286</td>\n","      <td>0.982292</td>\n","      <td>0.005621</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>21.888730</td>\n","      <td>0.087120</td>\n","      <td>2.239937</td>\n","      <td>0.064301</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>4</td>\n","      <td>80</td>\n","      <td>32</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 8,...</td>\n","      <td>0.810714</td>\n","      <td>0.821429</td>\n","      <td>0.817857</td>\n","      <td>0.751786</td>\n","      <td>0.800446</td>\n","      <td>0.028358</td>\n","      <td>4</td>\n","      <td>0.981548</td>\n","      <td>0.981548</td>\n","      <td>0.977976</td>\n","      <td>0.978571</td>\n","      <td>0.979911</td>\n","      <td>0.001650</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>22.546687</td>\n","      <td>0.050209</td>\n","      <td>2.223869</td>\n","      <td>0.115347</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>4</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 32...</td>\n","      <td>0.785714</td>\n","      <td>0.808929</td>\n","      <td>0.857143</td>\n","      <td>0.755357</td>\n","      <td>0.801786</td>\n","      <td>0.037180</td>\n","      <td>3</td>\n","      <td>0.993452</td>\n","      <td>0.990476</td>\n","      <td>0.986310</td>\n","      <td>0.986310</td>\n","      <td>0.989137</td>\n","      <td>0.003017</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>22.122966</td>\n","      <td>0.162522</td>\n","      <td>2.236358</td>\n","      <td>0.071473</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>4</td>\n","      <td>16</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 4,...</td>\n","      <td>0.776786</td>\n","      <td>0.787500</td>\n","      <td>0.808929</td>\n","      <td>0.744643</td>\n","      <td>0.779464</td>\n","      <td>0.023197</td>\n","      <td>18</td>\n","      <td>0.975000</td>\n","      <td>0.971429</td>\n","      <td>0.972619</td>\n","      <td>0.982738</td>\n","      <td>0.975446</td>\n","      <td>0.004402</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>23.612614</td>\n","      <td>0.264461</td>\n","      <td>2.298483</td>\n","      <td>0.085842</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>8</td>\n","      <td>16</td>\n","      <td>50</td>\n","      <td>8</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 8,...</td>\n","      <td>0.789286</td>\n","      <td>0.785714</td>\n","      <td>0.792857</td>\n","      <td>0.698214</td>\n","      <td>0.766518</td>\n","      <td>0.039516</td>\n","      <td>20</td>\n","      <td>0.980952</td>\n","      <td>0.978571</td>\n","      <td>0.975595</td>\n","      <td>0.963690</td>\n","      <td>0.974702</td>\n","      <td>0.006635</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>20.859725</td>\n","      <td>0.269622</td>\n","      <td>2.289628</td>\n","      <td>0.140150</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>50</td>\n","      <td>4</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 4,...</td>\n","      <td>0.794643</td>\n","      <td>0.757143</td>\n","      <td>0.823214</td>\n","      <td>0.748214</td>\n","      <td>0.780804</td>\n","      <td>0.030050</td>\n","      <td>16</td>\n","      <td>0.949405</td>\n","      <td>0.979167</td>\n","      <td>0.963690</td>\n","      <td>0.979762</td>\n","      <td>0.968006</td>\n","      <td>0.012524</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>21.748636</td>\n","      <td>0.404088</td>\n","      <td>2.256362</td>\n","      <td>0.228395</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>50</td>\n","      <td>16</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 8,...</td>\n","      <td>0.810714</td>\n","      <td>0.782143</td>\n","      <td>0.808929</td>\n","      <td>0.773214</td>\n","      <td>0.793750</td>\n","      <td>0.016391</td>\n","      <td>6</td>\n","      <td>0.980357</td>\n","      <td>0.972619</td>\n","      <td>0.976190</td>\n","      <td>0.979167</td>\n","      <td>0.977083</td>\n","      <td>0.002991</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>26.461846</td>\n","      <td>0.456761</td>\n","      <td>2.294801</td>\n","      <td>0.241084</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>50</td>\n","      <td>16</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 8,...</td>\n","      <td>0.766071</td>\n","      <td>0.807143</td>\n","      <td>0.803571</td>\n","      <td>0.735714</td>\n","      <td>0.778125</td>\n","      <td>0.029298</td>\n","      <td>19</td>\n","      <td>0.992262</td>\n","      <td>0.985714</td>\n","      <td>0.975000</td>\n","      <td>0.978571</td>\n","      <td>0.982887</td>\n","      <td>0.006647</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>22.915899</td>\n","      <td>0.550586</td>\n","      <td>2.382524</td>\n","      <td>0.222180</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>32</td>\n","      <td>4</td>\n","      <td>50</td>\n","      <td>4</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 32...</td>\n","      <td>0.773214</td>\n","      <td>0.789286</td>\n","      <td>0.803571</td>\n","      <td>0.760714</td>\n","      <td>0.781696</td>\n","      <td>0.016189</td>\n","      <td>14</td>\n","      <td>0.992262</td>\n","      <td>0.980357</td>\n","      <td>0.984524</td>\n","      <td>0.977976</td>\n","      <td>0.983780</td>\n","      <td>0.005429</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>20.907099</td>\n","      <td>0.356301</td>\n","      <td>2.210489</td>\n","      <td>0.133289</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>30</td>\n","      <td>8</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 16...</td>\n","      <td>0.771429</td>\n","      <td>0.801786</td>\n","      <td>0.812500</td>\n","      <td>0.771429</td>\n","      <td>0.789286</td>\n","      <td>0.018254</td>\n","      <td>9</td>\n","      <td>0.992857</td>\n","      <td>0.980952</td>\n","      <td>0.976786</td>\n","      <td>0.973214</td>\n","      <td>0.980952</td>\n","      <td>0.007399</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>21.471331</td>\n","      <td>0.328946</td>\n","      <td>2.380853</td>\n","      <td>0.155427</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 4,...</td>\n","      <td>0.789286</td>\n","      <td>0.823214</td>\n","      <td>0.821429</td>\n","      <td>0.733929</td>\n","      <td>0.791964</td>\n","      <td>0.036125</td>\n","      <td>7</td>\n","      <td>0.986905</td>\n","      <td>0.979762</td>\n","      <td>0.972024</td>\n","      <td>0.983333</td>\n","      <td>0.980506</td>\n","      <td>0.005510</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>25.222079</td>\n","      <td>0.323781</td>\n","      <td>2.364163</td>\n","      <td>0.230997</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>80</td>\n","      <td>4</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 16...</td>\n","      <td>0.760714</td>\n","      <td>0.814286</td>\n","      <td>0.801786</td>\n","      <td>0.767857</td>\n","      <td>0.786161</td>\n","      <td>0.022459</td>\n","      <td>11</td>\n","      <td>0.994643</td>\n","      <td>0.963690</td>\n","      <td>0.979762</td>\n","      <td>0.983929</td>\n","      <td>0.980506</td>\n","      <td>0.011123</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>21.271167</td>\n","      <td>0.199996</td>\n","      <td>2.288914</td>\n","      <td>0.218400</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>50</td>\n","      <td>16</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 4,...</td>\n","      <td>0.812500</td>\n","      <td>0.848214</td>\n","      <td>0.835714</td>\n","      <td>0.744643</td>\n","      <td>0.810268</td>\n","      <td>0.039997</td>\n","      <td>1</td>\n","      <td>0.968452</td>\n","      <td>0.955357</td>\n","      <td>0.959524</td>\n","      <td>0.973810</td>\n","      <td>0.964286</td>\n","      <td>0.007254</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>26.825859</td>\n","      <td>0.378080</td>\n","      <td>2.344052</td>\n","      <td>0.249775</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>30</td>\n","      <td>8</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 16...</td>\n","      <td>0.796429</td>\n","      <td>0.794643</td>\n","      <td>0.800000</td>\n","      <td>0.733929</td>\n","      <td>0.781250</td>\n","      <td>0.027389</td>\n","      <td>15</td>\n","      <td>0.991071</td>\n","      <td>0.981548</td>\n","      <td>0.985714</td>\n","      <td>0.980952</td>\n","      <td>0.984821</td>\n","      <td>0.004048</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>21.282034</td>\n","      <td>0.281827</td>\n","      <td>2.272139</td>\n","      <td>0.163817</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>30</td>\n","      <td>32</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 8,...</td>\n","      <td>0.758929</td>\n","      <td>0.810714</td>\n","      <td>0.825000</td>\n","      <td>0.737500</td>\n","      <td>0.783036</td>\n","      <td>0.035992</td>\n","      <td>13</td>\n","      <td>0.982143</td>\n","      <td>0.984524</td>\n","      <td>0.975595</td>\n","      <td>0.981548</td>\n","      <td>0.980952</td>\n","      <td>0.003287</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>25.053010</td>\n","      <td>0.788125</td>\n","      <td>2.464785</td>\n","      <td>0.161068</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>4</td>\n","      <td>32</td>\n","      <td>30</td>\n","      <td>16</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 4,...</td>\n","      <td>0.753571</td>\n","      <td>0.823214</td>\n","      <td>0.846429</td>\n","      <td>0.737500</td>\n","      <td>0.790179</td>\n","      <td>0.045745</td>\n","      <td>8</td>\n","      <td>0.975595</td>\n","      <td>0.941071</td>\n","      <td>0.971429</td>\n","      <td>0.971429</td>\n","      <td>0.964881</td>\n","      <td>0.013851</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>22.031580</td>\n","      <td>0.384310</td>\n","      <td>2.454388</td>\n","      <td>0.135297</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>16</td>\n","      <td>4</td>\n","      <td>30</td>\n","      <td>16</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 16...</td>\n","      <td>0.791071</td>\n","      <td>0.826786</td>\n","      <td>0.828571</td>\n","      <td>0.760714</td>\n","      <td>0.801786</td>\n","      <td>0.028036</td>\n","      <td>2</td>\n","      <td>0.977976</td>\n","      <td>0.977381</td>\n","      <td>0.984524</td>\n","      <td>0.985714</td>\n","      <td>0.981399</td>\n","      <td>0.003750</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>28.296969</td>\n","      <td>0.644581</td>\n","      <td>2.580660</td>\n","      <td>0.338388</td>\n","      <td>5000</td>\n","      <td>50</td>\n","      <td>32</td>\n","      <td>16</td>\n","      <td>80</td>\n","      <td>16</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 32...</td>\n","      <td>0.771429</td>\n","      <td>0.782143</td>\n","      <td>0.812500</td>\n","      <td>0.755357</td>\n","      <td>0.780357</td>\n","      <td>0.020863</td>\n","      <td>17</td>\n","      <td>0.994643</td>\n","      <td>0.992857</td>\n","      <td>0.986310</td>\n","      <td>0.991071</td>\n","      <td>0.991220</td>\n","      <td>0.003104</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>27.182985</td>\n","      <td>0.523902</td>\n","      <td>2.373412</td>\n","      <td>0.216162</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>30</td>\n","      <td>8</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 16...</td>\n","      <td>0.805357</td>\n","      <td>0.789286</td>\n","      <td>0.796429</td>\n","      <td>0.750000</td>\n","      <td>0.785268</td>\n","      <td>0.021143</td>\n","      <td>12</td>\n","      <td>0.973810</td>\n","      <td>0.979762</td>\n","      <td>0.985119</td>\n","      <td>0.988095</td>\n","      <td>0.981696</td>\n","      <td>0.005445</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>24.840567</td>\n","      <td>1.533558</td>\n","      <td>2.143461</td>\n","      <td>0.490062</td>\n","      <td>5000</td>\n","      <td>29</td>\n","      <td>4</td>\n","      <td>32</td>\n","      <td>30</td>\n","      <td>4</td>\n","      <td>{'vocab_size': 5000, 'maxlen': 29, 'lstm2': 4,...</td>\n","      <td>0.792857</td>\n","      <td>0.821429</td>\n","      <td>0.792857</td>\n","      <td>0.769643</td>\n","      <td>0.794196</td>\n","      <td>0.018358</td>\n","      <td>5</td>\n","      <td>0.976786</td>\n","      <td>0.964881</td>\n","      <td>0.968452</td>\n","      <td>0.979167</td>\n","      <td>0.972321</td>\n","      <td>0.005855</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n","0       26.881984      0.329249  ...          0.982292         0.005621\n","1       21.888730      0.087120  ...          0.979911         0.001650\n","2       22.546687      0.050209  ...          0.989137         0.003017\n","3       22.122966      0.162522  ...          0.975446         0.004402\n","4       23.612614      0.264461  ...          0.974702         0.006635\n","5       20.859725      0.269622  ...          0.968006         0.012524\n","6       21.748636      0.404088  ...          0.977083         0.002991\n","7       26.461846      0.456761  ...          0.982887         0.006647\n","8       22.915899      0.550586  ...          0.983780         0.005429\n","9       20.907099      0.356301  ...          0.980952         0.007399\n","10      21.471331      0.328946  ...          0.980506         0.005510\n","11      25.222079      0.323781  ...          0.980506         0.011123\n","12      21.271167      0.199996  ...          0.964286         0.007254\n","13      26.825859      0.378080  ...          0.984821         0.004048\n","14      21.282034      0.281827  ...          0.980952         0.003287\n","15      25.053010      0.788125  ...          0.964881         0.013851\n","16      22.031580      0.384310  ...          0.981399         0.003750\n","17      28.296969      0.644581  ...          0.991220         0.003104\n","18      27.182985      0.523902  ...          0.981696         0.005445\n","19      24.840567      1.533558  ...          0.972321         0.005855\n","\n","[20 rows x 24 columns]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"pQRkWpat-gZ3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1593082167378,"user_tz":-120,"elapsed":1093850,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"7e555086-0bfd-4656-a358-a6eec75da901"},"source":["print(grid_result.best_score_)\n","print(grid_result.best_params_)\n","\n","# test_accuracy = grid.score(x_test_pad, test_target)\n","# print(test_accuracy)\n","print(grid_result)\n","\n","\n","# train_accuracy = grid.score(x_train_pad, train_target)\n","# print(train_accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.8102678507566452\n","{'vocab_size': 5000, 'maxlen': 50, 'lstm2': 4, 'lstm1': 4, 'embedding_dim': 50, 'dense1': 16}\n","RandomizedSearchCV(cv=4, error_score=nan,\n","                   estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fb9db8fb048>,\n","                   iid='deprecated', n_iter=20, n_jobs=-1,\n","                   param_distributions={'dense1': [4, 8, 16, 32],\n","                                        'embedding_dim': [50, 30, 80],\n","                                        'lstm1': [4, 8, 16, 32],\n","                                        'lstm2': [4, 8, 16, 32],\n","                                        'maxlen': [29, 50],\n","                                        'vocab_size': [5000]},\n","                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n","                   return_train_score=True, scoring=None, verbose=1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"emfAjjqFh-yp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1593082168918,"user_tz":-120,"elapsed":1095364,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"9f1aec83-ecd0-4c72-ae05-86683b19d6e6"},"source":["threshold = 0.5\n","predicted = grid.predict(x_train_pad)\n","\n","predicted = np.where(predicted > threshold, 1,0)\n","\n","\n","predicted  = np.reshape(predicted, len(predicted))\n","\n","\n","eval_predicted = grid.predict(x_test_pad)\n","\n","eval_predicted = np.where(eval_predicted > threshold, 1,0)\n","\n","eval_predicted  = np.reshape(eval_predicted, len(eval_predicted))\n","\n","# train acc\n","\n","no_predictions = len(predicted)\n","print(no_predictions)\n","\n","right = np.sum(predicted == train_target)\n","\n","training_accuracy = right/ no_predictions\n","print(\"Training Accuracy: \", training_accuracy)\n","\n","\n","# test acc\n","\n","no_predictions_eval = len(eval_predicted)\n","print(no_predictions_eval)\n","\n","right_eval = np.sum(eval_predicted == test_target)\n","\n","test_accuracy = right_eval/ no_predictions_eval\n","print(\"Test Accuracy: \", test_accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"layer_embedding_input_2:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 29).\n","2240\n","Training Accuracy:  0.971875\n","720\n","Test Accuracy:  0.7777777777777778\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n6oaJFs1imnH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"executionInfo":{"status":"ok","timestamp":1593082168919,"user_tz":-120,"elapsed":1095330,"user":{"displayName":"Clyde Tran","photoUrl":"","userId":"01612062056156838085"}},"outputId":"7888f8ad-8dd4-4d85-87a9-f1b48265f249"},"source":["from sklearn.linear_model import LogisticRegression\n","import sklearn.metrics as sk\n","\n","# loss, accuracy = grid.score(x_train_pad, train_target)\n","# print(\"Training Accuracy: {:.4f}\".format(accuracy))\n","# print(\"Training Loss: {:.4f}\".format(loss))\n","\n","\n","precision = sk.precision_score(train_target, predicted)\n","print(\"Training Precision: \", precision)\n","recall = sk.recall_score(train_target, predicted)\n","print(\"Training recall: \", recall)\n","roc_auc = sk.roc_auc_score(train_target, predicted)\n","print(\"Training roc_auc: \", roc_auc)\n","f1 = sk.f1_score(train_target, predicted)\n","print(\"Training f1: \", f1)\n","\n","\n","# loss, accuracy = grid.score(x_test_pad, test_target)\n","# print(\"Eval Accuracy: {:.4f}\".format(accuracy))\n","# print(\"Eval Loss: {:.4f}\".format(loss))\n","\n","\n","precision = sk.precision_score(test_target, eval_predicted)\n","print(\"Eval Precision: \", precision)\n","recall = sk.recall_score(test_target, eval_predicted)\n","print(\"Eval recall: \", recall)\n","roc_auc = sk.roc_auc_score(test_target, eval_predicted)\n","print(\"Eval roc_auc: \", roc_auc)\n","f1 = sk.f1_score(test_target, eval_predicted)\n","print(\"Eval f1: \", f1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training Precision:  0.9543245869776482\n","Training recall:  0.9839679358717435\n","Training roc_auc:  0.9730628729278202\n","Training f1:  0.9689195855944747\n","Eval Precision:  0.6763754045307443\n","Eval recall:  0.7769516728624535\n","Eval roc_auc:  0.7776110914201403\n","Eval f1:  0.7231833910034601\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dRQKzpHRi0sy","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","\n","def plot_history(history):\n","    acc = history.history['accuracy']\n","    val_acc = history.history['val_accuracy']\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    x = range(1, len(acc) + 1)\n","\n","    plt.figure(figsize=(12, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(x, acc, 'b', label='Training acc')\n","    plt.plot(x, val_acc, 'r', label='Validation acc')\n","    plt.title('Training and validation accuracy')\n","    plt.legend()\n","    plt.subplot(1, 2, 2)\n","    plt.plot(x, loss, 'b', label='Training loss')\n","    plt.plot(x, val_loss, 'r', label='Validation loss')\n","    plt.title('Training and validation loss')\n","    plt.legend()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KP3enJc9i2zs","colab_type":"code","colab":{}},"source":["plot_history(history) "],"execution_count":null,"outputs":[]}]}